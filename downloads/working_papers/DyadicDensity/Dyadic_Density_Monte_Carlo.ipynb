{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel density estimation for dyadic data\n",
    "_Bryan Graham - University of California - Berkeley_  \n",
    "_Fengshi Niu - University of California - Berkeley_  \n",
    "_James Powell - University of California - Berkeley_  \n",
    "\n",
    "June 2019\n",
    "\n",
    "This notebook replicates the Monte Carlo experiments reported in the paper _Kernel Density Estimation for Undirected Dyadic Data_. While that paper only studies the case of undirected data, the **dyadic_kden()** function introduced below can handle both directed and undirected data (the former of which is a topic of ongoing research.\n",
    "<br>\n",
    "<br>\n",
    "Please feel free to use and adapt this code for your own projects. We ask only that you cite our paper and the use of this codebase.\n",
    "<br>\n",
    "<br>\n",
    "This code is provided \"as is\" without implicit warranty. While we welcome bug reports or suggestions for improvements, we regret that we are not able to provide any support. You are on your own!\n",
    "\n",
    "#### Code citation:\n",
    "<br>\n",
    "Graham, Bryan S., Niu, Fengshi and Powell James. (2019). \"Kernel density estimation for dyadic data: Python Jupyter Notebook,\" (Version 1.0) [Computer program]. Available at http://bryangraham.github.io/econometrics/ (Accessed 18 March 2020)\n",
    "<br>\n",
    "<br>\n",
    "Graham, Bryan S., Niu, Fengshi and Powell James. (2019). \"Kernel density estimation for undirected dyadic data\" [Working Paper]. Available at http://bryangraham.github.io/econometrics/ (Accessed 18 March 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Python to plot all figures inline (i.e., not in a separate window)\n",
    "%matplotlib inline\n",
    "\n",
    "# Main scientific computing modules\n",
    "# Load library dependencies\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import math as math\n",
    "\n",
    "# Import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# networkx module for the analysis of network data\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory where graphics files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphics     = '/Users/bgraham/Dropbox/Research/Networks/DyadicRegression/Graphics/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyadic Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first block of code defines the **dyadic_kden()** function. It currently only supports one dimensional density estimation problems, but it can handle both undirected and directed data. It includes various options to plot your density estimate along with pointwise confidence bands. The main input to the function is a vector of dyadic outcomes. This vector must be a Panda series  object with a multi-index (see documentation in the function below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dyadic_kden(X, x_grid, h = None, smooth_factor = 1, directed = True, cov='DR_bc', spike = False, plot = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    AUTHOR: Bryan S. Graham, UC - Berkeley, bgraham@econ.berkeley.edu, January 2019\n",
    "    PYTHON 3.6 (Updated May 2019)\n",
    "    \n",
    "    This function computes dyadic kernel density estimates. A variety of standard error estimates, \n",
    "    as described further below, are available to be reported along with point estimates of the density \n",
    "    function. The estimator, as well as its theoretical properties, are described in \n",
    "    Graham, Niu and Powell (2019,\"Kernel density estimation for dyadic data\").\n",
    "        \n",
    "    N = number of agents\n",
    "    n = N(N-1), the number of *directed* dyads, or n = 0.5N(N-1), the number of *undirected* dyads\n",
    "    \n",
    "    \n",
    "    INPUTS:\n",
    "    -------\n",
    "    X              :  n-length Pandas series with outcome for each\n",
    "                      (directed) dyad as elements (needs to have MultiIndex)\n",
    "    x_grid         :  m x 1 grid of values at which to estimate f(x) (also a Pandas series) \n",
    "    h              :  Bandwidth (scalar); if none use Silverman's rule-of-thumb\n",
    "    smooth_factor  :  Blow-up or scale-down bandwidth by a multiplicative factor\n",
    "    directed       :  if True then assume N*(N-1) directed outcomes present, \n",
    "                      otherwise assume 0.5*N(N-1) undirected outcomes are present\n",
    "    cov            :  covariance matrix estimator - \n",
    "                      'ind', 'DR', 'DR_bc' are allowable choices (see below)\n",
    "    spike          :  If True then f(x) is assumed to be defined on positive part of real line\n",
    "                      with a (possible) spike at zero. If Spike == True the user should exclude\n",
    "                      x=0 from the x_grid. The program will then renormalize the estimated density\n",
    "                      to integrate to Pr(X>0) and separately estimate the mass point Pr(X=0)\n",
    "    plot           :  If True, then plot density estimate and confidence band                  \n",
    "    \n",
    "    \n",
    "    The three variance-covariance matrices are as described in Graham (forthcoming, \n",
    "    Handbook of Econometrics). 'ind' assumes independence across dyads (note this \n",
    "    corresponds to \"clustering\" on dyads in the directed case where each dyad is \n",
    "    observed twice); `DR' allows for dependence across dyads sharing an agent in \n",
    "    common. It corresponds to the usual Jackknife variance estimate of the leading \n",
    "    term in the asymptotic variance expression. 'DR_bc' is a bias-corrected variance estimate.\n",
    "    This estimate also includes \"higher order\" variance components.\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    --------\n",
    "    f_hat        : m x 1 vector of regression function estimates \n",
    "    vcov_f_hat   : m x m variance-covariance matrix \n",
    "    fig          : matlibplot figure object\n",
    "    Sigma1_bc    : bias-corrected estimate of Sigma1 (m vector)\n",
    "    Sigma2       : estimated of Sigma2 (m vector)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #-------------------------------------------------------------------#\n",
    "    #- STEP 1 : ORGANIZE DATA AND PREPARE FOR ESTIMATION               -#\n",
    "    #-------------------------------------------------------------------#\n",
    "    \n",
    "    # Check to make sure X is a pandas objects with multi-indices\n",
    "    if not isinstance(X.index, pd.core.index.MultiIndex):\n",
    "        print(\"X is not a Pandas Series with an (i,j) multi-index\")\n",
    "        return [None, None, None]\n",
    "    \n",
    "    # Get dataset dimensions and agent/dyad indices\n",
    "    n        = len(X)               # Number of directed/undirected dyads (n = N(N-1) or 0.5N(N-1))\n",
    "    i, j     = X.index.levels       # Agent-level indices associated with each dyad\n",
    "    agents   = set(i | j)           # Set of all unique agent-level indices (N elements)\n",
    "    N        = len(agents)          # Number of agents    \n",
    "    idx, jdx = list(X.index.names)  # Labels for i and j indices (e.g., 'exporter', 'importer')\n",
    "    \n",
    "    # Check to see if expected number of outcomes are present. Function\n",
    "    # only works with balanced datasets at the present time.\n",
    "    if directed and (n != N*(N-1)):\n",
    "        print(\"Number of directed outcomes does not equal N(N-1)\")\n",
    "        return [None, None, None]\n",
    "        \n",
    "    if not directed and (n != (N*(N-1) // 2)):\n",
    "        print(\"Number of undirected outcomes does not equal N(N-1)/2\")    \n",
    "        return [None, None, None]\n",
    "    \n",
    "    # Set bandwidth value if needed (Silverman's rule-of-thumb)\n",
    "    if h is None:\n",
    "        iqr   = np.subtract.reduce(np.percentile(X, [75,25]))  # inter-quartile range, returned as scalar\n",
    "        sigma = np.std(X, ddof=1)                              # standard deviation\n",
    "        A     = min(sigma, iqr/1.349)                          # pick smallest of two dispersion measures\n",
    "        h     = smooth_factor * 0.9 * A * n ** (-1/5.)         # Silverman's rule-of-thumb times additional\n",
    "                                                               # user-supplied \"smooth factor\"\n",
    "\n",
    "    #-------------------------------------------------------------------#\n",
    "    #- STEP 2 : COMPUTE KERNEL DENSITY ESTIMATES                       -#\n",
    "    #-------------------------------------------------------------------#\n",
    "       \n",
    "    # Sort data prior to estimation (speeds up variance-covariance calculation)\n",
    "    X.sort_index(level = [idx, jdx], inplace = True) \n",
    "    \n",
    "    # Initialize density estimate objects\n",
    "    m      = len(x_grid)               # number of grid points at which density is being estimates\n",
    "    k      = np.zeros((n,m))           # n x m matrix of kernel values\n",
    "    S_ij   = np.zeros((n,m))           # \"score\" values\n",
    "    f_hat  = np.zeros((m,))            # m-vector of density estimates\n",
    "    \n",
    "    for l in range(0,m):\n",
    "        x_l       = x_grid.iloc[[l]].values - X.values             # n x 1 vector of X_ij - x values\n",
    "        k[:,l]    = sp.stats.norm.pdf(x_l/h)/h                     # n x 1 vector of k((x - X_ij)/h) values\n",
    "        S_ij[:,l] = k[:,l]                                         # Summands of estimator\n",
    "        f_hat[l]  = np.mean(S_ij[:,l])                             # Kernel estimate of f(x)\n",
    "        S_ij[:,l] = (k[:,l] - f_hat[l])                            # Re-center n x 1 S_ij vector to be mean zero\n",
    "        \n",
    "    \n",
    "    #-------------------------------------------------------------------#\n",
    "    #- STEP 3 : COMPUTE VARIANCE-COVARIANCE MATRIX                     -#\n",
    "    #-------------------------------------------------------------------#\n",
    "       \n",
    "    if not directed:\n",
    "        \n",
    "        #-----------------------------------#\n",
    "        #- Case 1: Undirected outcome data -#\n",
    "        #-----------------------------------#\n",
    "        \n",
    "        # Compute estimates of Sigma1 and Sigma2, the covariance between \"score\"\n",
    "        # contributions with, respectively, one and two indices in common. \n",
    "        \n",
    "        s_bar2 = S_ij                    # s_bar2 is the same as S_ij calculate above (n x m)\n",
    "        s_bar1 = np.zeros((N,m))         # Initialize matrix for score Hajek projections                          \n",
    "        lt_ij  = np.tril_indices(N,-1)   # Indices of lower triangle of an N x N matrix      \n",
    "        \n",
    "        # Form scores and projection using S_ij \n",
    "        for l in range(0,m):\n",
    "            S_l_ij         = np.zeros((N,N))                                  # Form N x N symmetric matrix with\n",
    "            S_l_ij[lt_ij]  = s_bar2[:,l]                                      # s_bar2 \"scores\" as elements\n",
    "            S_l_sym        = (S_l_ij + S_l_ij.T)                              # Mirror lower triangle in upper triangle\n",
    "            s_bar1[:,l]    = (N/(N-1))*np.mean((S_l_sym + S_l_sym.T), axis=0) # Column sums of symmetric N x N matrix with scores\n",
    "                                                                              # (Hajek projection terms)\n",
    "        \n",
    "        # Compute Sigma2 (summation over N choose 2 terms)\n",
    "        Sigma2 = (s_bar2[:,l].reshape(-1,1).T @ s_bar2[:,l].reshape(-1,1))/n  # m x m\n",
    "        Sigma2 = np.diag(np.diag(Sigma2))                                     # m vector\n",
    "         \n",
    "        # Compute Sigma1 (summation over N terms)\n",
    "        Sigma1 = (s_bar1.T @ s_bar1)/N                                        # m x m\n",
    "        Sigma1 = np.diag(np.diag(Sigma1))                                     # m vector\n",
    "    \n",
    "    else:\n",
    "        #-----------------------------------#\n",
    "        #- Case 2: Directed outcome data   -#\n",
    "        #-----------------------------------#\n",
    "        \n",
    "        # Compute estimates of Sigma1 and Sigma2, the covariance between \"score\"\n",
    "        # contributions with, respectively, one and two indices in common. In\n",
    "        # the directed case we first need to symmetrize the score prior\n",
    "        # to the required U-statistic type variance calculations.\n",
    "        # Combine S_ij + S_ji to form symmetrized kernel \"s_bar2\"    \n",
    "        \n",
    "        s_bar2 = np.zeros((n // 2,m))    # Initialize matrix for symmetrized scores\n",
    "        s_bar1 = np.zeros((N,m))         # Initialize matrix for score projections\n",
    "        lt_ij  = np.tril_indices(N,-1)   # Indices of lower triangle of an N x N matrix \n",
    "        \n",
    "        # Form symmetrized scores using S_ij (non-symmetric \"Scores\")\n",
    "        for l in range(0,m):\n",
    "            S_l         = S_ij[:,l].reshape(((N-1),N), order=\"F\")           # reshape N(N-1) x 1 score into N-1 x N matrix\n",
    "            S_l_ij      = np.vstack([np.zeros((N,)),np.tril(S_l, k=0)])     # pad and rotate to compute S_ij + S_ji for \n",
    "            S_l_ji      = np.vstack([np.triu(S_l, k=1), np.zeros((N,))])    # all 0.5N(N-1) dyads\n",
    "            S_l_sym     = (S_l_ij + S_l_ji.T)/2                             # Lower triangle matrix with symmetric kernels\n",
    "            s_bar2[:,l] = S_l_sym[lt_ij]                                    # 0.5N(N-1) x 1 vector of symmetric scores for theta_k\n",
    "            s_bar1[:,l] = (N/(N-1))*np.mean((S_l_sym + S_l_sym.T), axis=0)  # Column sums of symmetric N x N matrix with scores\n",
    "                                                                            # (Hajek projection terms)\n",
    "        \n",
    "        # Compute Sigma2 (summation over N choose 2 terms)\n",
    "        Sigma2 = 2*(s_bar2.T @ s_bar2)/n                                   # m x m\n",
    "        Sigma2 = np.diag(np.diag(Sigma2)) \n",
    "         \n",
    "        # Compute Sigma1 (summation over N terms)\n",
    "        Sigma1 = (s_bar1.T @ s_bar1)/N                                     # m x m\n",
    "        Sigma1 = np.diag(np.diag(Sigma1)) \n",
    "      \n",
    "    # Bias correct estimate of Sigma1 as in Efron/Stein \n",
    "    # Sigma1 = ((N-1)/(N-2))*(Sigma1 - Sigma2/(N-1))\n",
    "    \n",
    "    # Compute the asymptotic variance-covariance matrix \n",
    "    # according to specificed method\n",
    "    \n",
    "    if cov == 'ind':\n",
    "        # Assume independence across dyads\n",
    "        vcov_f_hat = (2/(N-1))*(Sigma2)/N\n",
    "    elif cov == 'DR':\n",
    "        # Dependence across dyads allowed, only leading variance term retained\n",
    "        vcov_f_hat = 4*(Sigma1)/N\n",
    "    else:\n",
    "        # Dependence across dyads allowed, both variance terms retained\n",
    "        vcov_f_hat = 4*((Sigma1 - 0.5*Sigma2/(N-1)))/N\n",
    "        \n",
    "        # Use eigendecomposition to ensure variance matrix is positive\n",
    "        # definite\n",
    "        [L, Q] = np.linalg.eig(vcov_f_hat)\n",
    "        if not np.all(L>0):               # check for negative eigenvalues\n",
    "            L[L<0] = 0                    # remove negative eigenvalues\n",
    "            L      = np.diag(L)\n",
    "            iQ     = np.linalg.inv(Q)\n",
    "            vcov_f_hat = Q @ L @ iQ       # positive definite matrix\n",
    "    \n",
    "    # Form bias-corrected estimates of Sigma1 variance term\n",
    "    # (useful for Monte Carlo analysis and other comparisons)\n",
    "    Sigma1_bc = ((N-1)/(N-2))*(Sigma1 - Sigma2/(N-1))\n",
    "        \n",
    "    #-------------------------------------------------------------------#\n",
    "    #- STEP 4 : PLOT DENSITY ESTIMATE WITH POINTWISE CONFIDENCE BAND   -#\n",
    "    #-------------------------------------------------------------------#\n",
    "    \n",
    "    if plot:\n",
    "        \n",
    "        # Compute pointwise 95 percent confidence band\n",
    "        l_cib = f_hat - 1.96*np.sqrt(np.diag(vcov_f_hat))\n",
    "        u_cib = f_hat + 1.96*np.sqrt(np.diag(vcov_f_hat))\n",
    "        \n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(9, 6))\n",
    "        plt.sca(ax)  \n",
    "\n",
    "        # Adjust for spike at zero if relevant\n",
    "        if spike:\n",
    "            fraction_positive = np.count_nonzero(X)/(N*(N-1))\n",
    "            f_hat = f_hat*fraction_positive\n",
    "            l_cib = l_cib*fraction_positive\n",
    "            u_cib = u_cib*fraction_positive\n",
    "            plt.xlim([-0.1, x_grid.iloc[:].max()])     # plot mass at zero & rescale density \n",
    "            plt.ylim([x_grid.iloc[:].min(), \\\n",
    "                      np.max(u_cib) + 0.1*np.abs(np.max(u_cib))])\n",
    "            ax2 = ax.twinx()                           # instantiate a second axes that shares the same x-axis\n",
    "            ax2.set_ylabel('Mass at zero, ' + r'$\\Pr\\left(X=0\\right)$', fontsize=16)\n",
    "            ax2.axvline(x=0, ymin=0, ymax=1-fraction_positive, color = '#ED4E33', linewidth = 10, alpha=0.3) \n",
    "        else:\n",
    "            plt.xlim([x_grid.iloc[:].min(), x_grid.iloc[:].max()])\n",
    "            plt.ylim([0, \\\n",
    "                      np.max(u_cib) + 0.1*np.abs(np.max(u_cib))])\n",
    "        \n",
    "        ax.plot(x_grid, f_hat, color = '#FDB515', linewidth = 2)\n",
    "        ax.plot(x_grid, l_cib, color = '#003262', linewidth = 2, linestyle='dashed')\n",
    "        ax.plot(x_grid, u_cib, color = '#003262', linewidth = 2, linestyle='dashed')\n",
    "\n",
    "        ax.fill_between(x_grid.iloc[:], l_cib,  u_cib,  facecolor='#003262', alpha=0.2)\n",
    "        ax.fill_between(x_grid.iloc[:], f_hat,          facecolor='#FDB515', alpha=0.2)\n",
    "                    \n",
    "        # Add title and axis labels\n",
    "        from matplotlib import rc\n",
    "        rc('text', usetex=True)\n",
    "        plt.title('Kernel Density Estimate', fontsize=16)\n",
    "        ax.set_xlabel(X.name + ', x', fontsize=16)\n",
    "        ax.set_ylabel('Density, ' + r'$f(x)$', fontsize=16)\n",
    "        \n",
    "        # Clean up the plot, add frames etc.\n",
    "        ax.patch.set_facecolor('#DDD5C7')            # Color of background\n",
    "        ax.patch.set_alpha(0.35)                     # Translucency of background\n",
    "        ax.grid(False)                               # Remove gridlines from plot\n",
    "\n",
    "        # Add frame around plot\n",
    "        for spine in ['left','right','top','bottom']:\n",
    "            ax.spines[spine].set_visible(True)\n",
    "            ax.spines[spine].set_color('k')\n",
    "            ax.spines[spine].set_linewidth(2)\n",
    "    else:\n",
    "        fig = None\n",
    "            \n",
    "    return [f_hat, vcov_f_hat, fig, Sigma1_bc, Sigma2]            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block of code executes a small Monte Carlo experiment. For each Monte Carlo replication, a total $ n=\\binom{N}{2} $ outcomes of the form $Y_{ij}=A_{i}A_{j}+V_{ij} $ are generated. Here the $\\left\\{ A_{i}\\right\\} _{i=1}^{N}$ are iid random variables which equal -1 with probability $q$ and 1 with probability $1-q$. The  $\\left\\{ V_{ij}\\right\\} _{i<j}$ are i.i.d. standard normal.\n",
    "<br>\n",
    "<br>\n",
    "We estimate $f_{Y}(1.645)$. We check the coverage properties of confidence intervals which ignore the presence of dependence across dyads sharing an agent in common as well as those constructed using the variance estimator proposed in the paper (which does take into account dyadic dependence).\n",
    "<br>\n",
    "<br>\n",
    "For the bandwidth we use the MSE-optimal one derived in the Graham, Niu and Powell (2019). This is an oracle bandwidth choice and not feasible (of course) in practice.\n",
    "<br>\n",
    "<br>\n",
    "We begin with some \"pencil and paper\" calculations which summarize the main features of our Monte Carlo designs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "N = 100\n",
      "f(w):       0.18537581\n",
      "Bandwidth:  0.24961154\n",
      "Bias:       -0.00325281\n",
      "ase:        0.01172446\n",
      "stddev, T1: 0.00975396\n",
      "stddev, T3: 0.00650563\n",
      "\n",
      "----------------------------------------\n",
      "N = 400\n",
      "f(w):       0.18537581\n",
      "Bandwidth:  0.14314795\n",
      "Bias:       -0.00106979\n",
      "ase:        0.00534278\n",
      "stddev, T1: 0.00489565\n",
      "stddev, T3: 0.00213959\n",
      "\n",
      "Bandwidth:  0.00342078\n",
      "Bias:       -0.00000061\n",
      "ase:        0.01468107\n",
      "stddev, T1: 0.00489565\n",
      "stddev, T3: 0.01384075\n",
      "\n",
      "----------------------------------------\n",
      "N = 1600\n",
      "f(w):       0.18537581\n",
      "Bandwidth:  0.08218603\n",
      "Bias:       -0.00035264\n",
      "ase:        0.00254962\n",
      "stddev, T1: 0.00245013\n",
      "stddev, T3: 0.00070527\n",
      "\n",
      "Bandwidth:  0.00085520\n",
      "Bias:       -0.00000004\n",
      "ase:        0.00733518\n",
      "stddev, T1: 0.00245013\n",
      "stddev, T3: 0.00691388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "R2 = 1/(2*np.sqrt(math.pi))\n",
    "K2 = 1\n",
    "\n",
    "q = 1/3\n",
    "w = 1.645\n",
    "\n",
    "f_Am1 = q*sp.stats.norm.pdf(w-1) + (1-q)*sp.stats.norm.pdf(w+1)\n",
    "f_Ap1 = q*sp.stats.norm.pdf(w+1) + (1-q)*sp.stats.norm.pdf(w-1)\n",
    "f_0 = (q**2 + (1-q)**2)*sp.stats.norm.pdf(w-1) + 2*q*(1-q)*sp.stats.norm.pdf(w+1)\n",
    "D2f = (q**2 + (1-q)**2)*((w-1)**2-1)*sp.stats.norm.pdf(w-1) + 2*q*(1-q)**((w+1)**2-1)*sp.stats.norm.pdf(w+1)\n",
    "B = 0.5*D2f*K2\n",
    "\n",
    "OMEGA1 = q*f_Am1**2 + (1-q)*f_Ap1**2 - f_0**2\n",
    "OMEGA2 = f_0*R2\n",
    "\n",
    "sample_sizes = [100, 400, 1600]\n",
    "\n",
    "bandwidths = []\n",
    "\n",
    "for N in sample_sizes:\n",
    "    \n",
    "    n      = N*(N-1) // 2\n",
    "    h_opt  = (0.25*(OMEGA2/B**2)*(1/n))**(1/5)\n",
    "        \n",
    "    if N != 100:\n",
    "        h_alt = bandwidths[0][0]*((sample_sizes[0]*(sample_sizes[0]-1) // 2)**(1/5))*(1/N)\n",
    "        bw     = [h_opt, h_alt]\n",
    "    else:\n",
    "        bw     = [h_opt]\n",
    "    \n",
    "    bandwidths.append(bw)\n",
    "    \n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"N = \" + '{0:.0f}'.format(N))\n",
    "    print(\"f(w):       \" + '{0:.8f}'.format(f_0))\n",
    "    \n",
    "    for h in bw:\n",
    "        print(\"Bandwidth:  \" + '{0:.8f}'.format(h))\n",
    "        print(\"Bias:       \" + '{0:.8f}'.format((h**2)*B))\n",
    "        print(\"ase:        \" + '{0:.8f}'.format(np.sqrt(2*OMEGA1*(N-2)/n + OMEGA2/(n*h))))\n",
    "        print(\"stddev, T1: \" + '{0:.8f}'.format(np.sqrt(2*OMEGA1*(N-2)/n)))\n",
    "        print(\"stddev, T3: \" + '{0:.8f}'.format(np.sqrt(OMEGA2/(n*h))))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform 1000 Monte Carlo simulations for $N=100,400$ and $1,600$. The paper summarizes these experiments, but the main gist is that the theory does a pretty good job of approximating finite sample behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 100, Time required f/ MC rep  50 of 1000: 0.031723976135253906\n",
      "N = 100, Time required f/ MC rep  100 of 1000: 0.028885841369628906\n",
      "N = 100, Time required f/ MC rep  150 of 1000: 0.028673171997070312\n",
      "N = 100, Time required f/ MC rep  200 of 1000: 0.02912306785583496\n",
      "N = 100, Time required f/ MC rep  250 of 1000: 0.027776002883911133\n",
      "N = 100, Time required f/ MC rep  300 of 1000: 0.028100967407226562\n",
      "N = 100, Time required f/ MC rep  350 of 1000: 0.0365910530090332\n",
      "N = 100, Time required f/ MC rep  400 of 1000: 0.02780294418334961\n",
      "N = 100, Time required f/ MC rep  450 of 1000: 0.027915000915527344\n",
      "N = 100, Time required f/ MC rep  500 of 1000: 0.027637004852294922\n",
      "N = 100, Time required f/ MC rep  550 of 1000: 0.03133797645568848\n",
      "N = 100, Time required f/ MC rep  600 of 1000: 0.03060770034790039\n",
      "N = 100, Time required f/ MC rep  650 of 1000: 0.028181076049804688\n",
      "N = 100, Time required f/ MC rep  700 of 1000: 0.032354116439819336\n",
      "N = 100, Time required f/ MC rep  750 of 1000: 0.07172369956970215\n",
      "N = 100, Time required f/ MC rep  800 of 1000: 0.03180098533630371\n",
      "N = 100, Time required f/ MC rep  850 of 1000: 0.02866816520690918\n",
      "N = 100, Time required f/ MC rep  900 of 1000: 0.02851724624633789\n",
      "N = 100, Time required f/ MC rep  950 of 1000: 0.029108047485351562\n",
      "N = 100, Time required f/ MC rep  1000 of 1000: 0.0283200740814209\n",
      "------------------------------------\n",
      "RESULTS FOR N = 100\n",
      "------------------------------------\n",
      "\n",
      "Bandwidth:  0.00085520\n",
      "Number of times kernel estimate of the density was successfully computed\n",
      "1000.0\n",
      "\n",
      "Monte Carlo summary statistics for density estimation\n",
      "\n",
      "              Bias       StdErr  Coverage (ind)  Coverage (1st)  \\\n",
      "count  1000.000000  1000.000000     1000.000000     1000.000000   \n",
      "mean     -0.001981     0.017377        0.678000        0.994000   \n",
      "std       0.011288     0.001999        0.467477        0.077266   \n",
      "min      -0.032590     0.011150        0.000000        0.000000   \n",
      "25%      -0.010066     0.015973        0.000000        1.000000   \n",
      "50%      -0.002800     0.017313        1.000000        1.000000   \n",
      "75%       0.004874     0.018658        1.000000        1.000000   \n",
      "max       0.053597     0.025575        1.000000        1.000000   \n",
      "\n",
      "       Coverage (FG)  Squared Error  \n",
      "count    1000.000000   1.000000e+03  \n",
      "mean        0.995000   1.312085e-04  \n",
      "std         0.070569   1.958883e-04  \n",
      "min         0.000000   6.847198e-11  \n",
      "25%         1.000000   1.327512e-05  \n",
      "50%         1.000000   6.136814e-05  \n",
      "75%         1.000000   1.769568e-04  \n",
      "max         1.000000   2.872673e-03  \n",
      "\n",
      "Root mean squared error (RMSE) of density estimate\n",
      "0.011454627171287252\n",
      "\n",
      "\n",
      "Robust estimate of standard deviation of density estimate\n",
      "0.01124856860428507\n",
      "\n",
      "N = 400, Time required f/ MC rep  50 of 1000: 0.4865400791168213\n",
      "N = 400, Time required f/ MC rep  100 of 1000: 0.4940187931060791\n",
      "N = 400, Time required f/ MC rep  150 of 1000: 0.46964573860168457\n",
      "N = 400, Time required f/ MC rep  200 of 1000: 0.5016388893127441\n",
      "N = 400, Time required f/ MC rep  250 of 1000: 0.47521519660949707\n",
      "N = 400, Time required f/ MC rep  300 of 1000: 0.486422061920166\n",
      "N = 400, Time required f/ MC rep  350 of 1000: 0.44345712661743164\n",
      "N = 400, Time required f/ MC rep  400 of 1000: 0.4832949638366699\n",
      "N = 400, Time required f/ MC rep  450 of 1000: 0.4806370735168457\n",
      "N = 400, Time required f/ MC rep  500 of 1000: 0.4871790409088135\n",
      "N = 400, Time required f/ MC rep  550 of 1000: 0.44438695907592773\n",
      "N = 400, Time required f/ MC rep  600 of 1000: 0.4900481700897217\n",
      "N = 400, Time required f/ MC rep  650 of 1000: 0.5050539970397949\n",
      "N = 400, Time required f/ MC rep  700 of 1000: 0.48357701301574707\n",
      "N = 400, Time required f/ MC rep  750 of 1000: 0.43505406379699707\n",
      "N = 400, Time required f/ MC rep  800 of 1000: 0.4940040111541748\n",
      "N = 400, Time required f/ MC rep  850 of 1000: 0.48210883140563965\n",
      "N = 400, Time required f/ MC rep  900 of 1000: 0.4882199764251709\n",
      "N = 400, Time required f/ MC rep  950 of 1000: 0.4940967559814453\n",
      "N = 400, Time required f/ MC rep  1000 of 1000: 0.4925801753997803\n",
      "------------------------------------\n",
      "RESULTS FOR N = 400\n",
      "------------------------------------\n",
      "\n",
      "Bandwidth:  0.00085520\n",
      "Number of times kernel estimate of the density was successfully computed\n",
      "1000.0\n",
      "\n",
      "Monte Carlo summary statistics for density estimation\n",
      "\n",
      "              Bias       StdErr  Coverage (ind)  Coverage (1st)  \\\n",
      "count  1000.000000  1000.000000     1000.000000     1000.000000   \n",
      "mean     -0.000657     0.006780        0.551000        0.979000   \n",
      "std       0.005118     0.000495        0.497641        0.143456   \n",
      "min      -0.015757     0.005391        0.000000        0.000000   \n",
      "25%      -0.004153     0.006435        0.000000        1.000000   \n",
      "50%      -0.001023     0.006757        1.000000        1.000000   \n",
      "75%       0.002589     0.007114        1.000000        1.000000   \n",
      "max       0.018564     0.008257        1.000000        1.000000   \n",
      "\n",
      "       Coverage (FG)  Squared Error  \n",
      "count    1000.000000   1.000000e+03  \n",
      "mean        0.987000   2.660397e-05  \n",
      "std         0.113331   3.844124e-05  \n",
      "min         0.000000   1.179685e-10  \n",
      "25%         1.000000   3.159703e-06  \n",
      "50%         1.000000   1.285319e-05  \n",
      "75%         1.000000   3.210918e-05  \n",
      "max         1.000000   3.446120e-04  \n",
      "\n",
      "Root mean squared error (RMSE) of density estimate\n",
      "0.0051579033894291\n",
      "\n",
      "\n",
      "Robust estimate of standard deviation of density estimate\n",
      "0.005115247804830132\n",
      "\n",
      "Bandwidth:  0.00085520\n",
      "Number of times kernel estimate of the density was successfully computed\n",
      "1000.0\n",
      "\n",
      "Monte Carlo summary statistics for density estimation\n",
      "\n",
      "              Bias       StdErr  Coverage (ind)  Coverage (1st)  \\\n",
      "count  1000.000000  1000.000000      1000.00000          1000.0   \n",
      "mean      0.000344     0.036784         0.94200             1.0   \n",
      "std       0.014260     0.002254         0.23386             0.0   \n",
      "min      -0.042340     0.030742         0.00000             1.0   \n",
      "25%      -0.009526     0.035250         1.00000             1.0   \n",
      "50%      -0.000169     0.036696         1.00000             1.0   \n",
      "75%       0.009899     0.038316         1.00000             1.0   \n",
      "max       0.046462     0.044321         1.00000             1.0   \n",
      "\n",
      "       Coverage (FG)  Squared Error  \n",
      "count         1000.0   1.000000e+03  \n",
      "mean             1.0   2.032558e-04  \n",
      "std              0.0   2.844990e-04  \n",
      "min              1.0   1.059388e-09  \n",
      "25%              1.0   2.020476e-05  \n",
      "50%              1.0   9.576877e-05  \n",
      "75%              1.0   2.657858e-04  \n",
      "max              1.0   2.158716e-03  \n",
      "\n",
      "Root mean squared error (RMSE) of density estimate\n",
      "0.014256780972596998\n",
      "\n",
      "\n",
      "Robust estimate of standard deviation of density estimate\n",
      "0.013846633089653624\n",
      "\n",
      "N = 1600, Time required f/ MC rep  50 of 1000: 8.020231008529663\n",
      "N = 1600, Time required f/ MC rep  100 of 1000: 7.839331150054932\n",
      "N = 1600, Time required f/ MC rep  150 of 1000: 7.768975734710693\n",
      "N = 1600, Time required f/ MC rep  200 of 1000: 7.775248289108276\n",
      "N = 1600, Time required f/ MC rep  250 of 1000: 7.82227897644043\n",
      "N = 1600, Time required f/ MC rep  300 of 1000: 7.811203241348267\n",
      "N = 1600, Time required f/ MC rep  350 of 1000: 7.795573949813843\n",
      "N = 1600, Time required f/ MC rep  400 of 1000: 7.793040990829468\n",
      "N = 1600, Time required f/ MC rep  450 of 1000: 7.793171167373657\n",
      "N = 1600, Time required f/ MC rep  500 of 1000: 7.794370174407959\n",
      "N = 1600, Time required f/ MC rep  550 of 1000: 7.820105075836182\n",
      "N = 1600, Time required f/ MC rep  600 of 1000: 7.900691032409668\n",
      "N = 1600, Time required f/ MC rep  650 of 1000: 7.9503889083862305\n",
      "N = 1600, Time required f/ MC rep  700 of 1000: 7.814295053482056\n",
      "N = 1600, Time required f/ MC rep  750 of 1000: 8.072205066680908\n",
      "N = 1600, Time required f/ MC rep  800 of 1000: 7.886322021484375\n",
      "N = 1600, Time required f/ MC rep  850 of 1000: 7.816067934036255\n",
      "N = 1600, Time required f/ MC rep  900 of 1000: 7.803237199783325\n",
      "N = 1600, Time required f/ MC rep  950 of 1000: 7.854228973388672\n",
      "N = 1600, Time required f/ MC rep  1000 of 1000: 8.250257968902588\n",
      "------------------------------------\n",
      "RESULTS FOR N = 1600\n",
      "------------------------------------\n",
      "\n",
      "Bandwidth:  0.00085520\n",
      "Number of times kernel estimate of the density was successfully computed\n",
      "1000.0\n",
      "\n",
      "Monte Carlo summary statistics for density estimation\n",
      "\n",
      "              Bias       StdErr  Coverage (ind)  Coverage (1st)  \\\n",
      "count  1000.000000  1000.000000     1000.000000     1000.000000   \n",
      "mean     -0.000372     0.002778        0.390000        0.961000   \n",
      "std       0.002475     0.000136        0.487994        0.193692   \n",
      "min      -0.006630     0.002347        0.000000        0.000000   \n",
      "25%      -0.002007     0.002687        0.000000        1.000000   \n",
      "50%      -0.000601     0.002777        0.000000        1.000000   \n",
      "75%       0.001162     0.002862        1.000000        1.000000   \n",
      "max       0.009937     0.003224        1.000000        1.000000   \n",
      "\n",
      "       Coverage (FG)  Squared Error  \n",
      "count    1000.000000   1.000000e+03  \n",
      "mean        0.967000   6.257294e-06  \n",
      "std         0.178726   8.666497e-06  \n",
      "min         0.000000   1.285056e-13  \n",
      "25%         1.000000   7.464284e-07  \n",
      "50%         1.000000   2.878753e-06  \n",
      "75%         1.000000   8.688270e-06  \n",
      "max         1.000000   9.873986e-05  \n",
      "\n",
      "Root mean squared error (RMSE) of density estimate\n",
      "0.002501458441429184\n",
      "\n",
      "\n",
      "Robust estimate of standard deviation of density estimate\n",
      "0.002483792620122871\n",
      "\n",
      "Bandwidth:  0.00085520\n",
      "Number of times kernel estimate of the density was successfully computed\n",
      "1000.0\n",
      "\n",
      "Monte Carlo summary statistics for density estimation\n",
      "\n",
      "              Bias       StdErr  Coverage (ind)  Coverage (1st)  \\\n",
      "count  1000.000000  1000.000000     1000.000000          1000.0   \n",
      "mean      0.000094     0.018391        0.929000             1.0   \n",
      "std       0.007353     0.000576        0.256953             0.0   \n",
      "min      -0.023226     0.016778        0.000000             1.0   \n",
      "25%      -0.005112     0.017977        1.000000             1.0   \n",
      "50%       0.000137     0.018422        1.000000             1.0   \n",
      "75%       0.004867     0.018779        1.000000             1.0   \n",
      "max       0.024656     0.020113        1.000000             1.0   \n",
      "\n",
      "       Coverage (FG)  Squared Error  \n",
      "count         1000.0   1.000000e+03  \n",
      "mean             1.0   5.401781e-05  \n",
      "std              0.0   7.693825e-05  \n",
      "min              1.0   2.370105e-10  \n",
      "25%              1.0   5.418434e-06  \n",
      "50%              1.0   2.466578e-05  \n",
      "75%              1.0   6.850897e-05  \n",
      "max              1.0   6.079176e-04  \n",
      "\n",
      "Root mean squared error (RMSE) of density estimate\n",
      "0.007349680649296502\n",
      "\n",
      "\n",
      "Robust estimate of standard deviation of density estimate\n",
      "0.007246470712923591\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of Monte Carlo replications\n",
    "B      = 1000\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(361)\n",
    "\n",
    "design = 0\n",
    "\n",
    "for N in sample_sizes:\n",
    "    \n",
    "    num_h = len(bandwidths[design])                # number of bandwidths for current design\n",
    "    Simulation_Results = np.zeros((B,7*num_h))     # matrix to store results in form current design\n",
    "    \n",
    "    \n",
    "    # ------------------------------------ #\n",
    "    # PERFORM B MONTE CARLO REPLICATIONS - #\n",
    "    # ------------------------------------ #\n",
    "\n",
    "    for b in range(0,B):\n",
    "        \n",
    "        # Start timer\n",
    "        start = time.time()\n",
    "\n",
    "        # Generate simulated sample\n",
    "        A = -1 + 2*(np.random.uniform(0,1,N)>=q)\n",
    "        Y = []\n",
    "\n",
    "        for i, j in it.combinations(range(0,N), 2):\n",
    "            Y.append([A[i]*A[j] + np.random.normal(0, 1), i, j])\n",
    "    \n",
    "        Y = pd.DataFrame(Y, columns=['Y', 'i', 'j'])    \n",
    "        Y = Y.set_index(['i', 'j'], drop = True)  # Set dataframe multi-index\n",
    "        Y = Y.iloc[:,0]\n",
    "        \n",
    "        i = 0\n",
    "\n",
    "        for bw in bandwidths[design]:\n",
    "        \n",
    "            try:\n",
    "                # compute kernel density estimate, confidence intervals etc.\n",
    "                [f_hat, vcov_hat, _, Sigma1_bc, Sigma2] = dyadic_kden(Y, pd.Series([w]), h = bw, smooth_factor = 1, \\\n",
    "                                                                      directed = False, cov='DR_bc', spike=False, plot = False)\n",
    "         \n",
    "                Simulation_Results[b,i*7 + 0] = True\n",
    "                Simulation_Results[b,i*7 + 1] = f_hat[0] - f_0                     # Record bias of density estimate\n",
    "                Simulation_Results[b,i*7 + 2] = vcov_hat[0,0]**(1/2)               # Record standard error estimate (FG)\n",
    "            \n",
    "                # Check coverage of 95 percent Wald asymptotic confidence interval (ignoring dyadic dependence)\n",
    "                Simulation_Results[b,i*7 + 3] = (f_hat[0] - 1.96*((2/(N-1))*(Sigma2)/N)**(1/2) <= f_0) & \\\n",
    "                                                (f_hat[0] + 1.96*((2/(N-1))*(Sigma2)/N)**(1/2) >= f_0)\n",
    "            \n",
    "                # Check coverage of 95 percent Wald asymptotic confidence interval (accounting for dyadic dependence)\n",
    "                Simulation_Results[b,i*7 + 4] = (f_hat[0] - 1.96*(4*Sigma1_bc[0,0]/N)**(1/2) <= f_0) & \\\n",
    "                                                (f_hat[0] + 1.96*(4*Sigma1_bc[0,0]/N)**(1/2) >= f_0)\n",
    "            \n",
    "                # Check coverage of 95 percent Wald asymptotic confidence interval (FG standard errors)\n",
    "                Simulation_Results[b,i*7 + 5] = (f_hat[0] - 1.96*Simulation_Results[b,i*7 + 2] <= f_0) & \\\n",
    "                                                (f_hat[0] + 1.96*Simulation_Results[b,i*7 + 2] >= f_0)\n",
    "            \n",
    "                # Squared error loss\n",
    "                Simulation_Results[b,i*7 + 6] = (f_hat[0]-f_0)**2\n",
    "        \n",
    "            except:        \n",
    "                Simulation_Results[b,i*7 + 0] = False\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "        end = time.time()\n",
    "        if (b+1) % 50 == 0:\n",
    "            print(\"N = \"+ str(N)+\", Time required f/ MC rep  \" + \\\n",
    "                  str(b+1) + \" of \" + str(B) + \": \" + str(end-start))          \n",
    "                     \n",
    "    # ------------------------------------------------ #\n",
    "    # - SUMMARIZE RESULTS OF MONTE CARLO EXPERIMENT  - #\n",
    "    # ------------------------------------------------ #\n",
    "    \n",
    "    print(\"------------------------------------\")\n",
    "    print(\"RESULTS FOR N = \" + str(N)) \n",
    "    print(\"------------------------------------\")\n",
    "    print(\"\")\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for bw in bandwidths[design]:\n",
    "        print(\"Bandwidth:  \" + '{0:.8f}'.format(h))\n",
    "        print(\"Number of times kernel estimate of the density was successfully computed\")\n",
    "        print(np.sum(Simulation_Results[:,0], axis = 0)) \n",
    "                      \n",
    "        # find simulation replicates where computation was successful\n",
    "        b = np.where(Simulation_Results[:,i*7 + 0])[0]\n",
    "        SimRes = Simulation_Results[b,:]\n",
    "                      \n",
    "        # create Pandas dataframe with Monte Carlo results\n",
    "        SR=pd.DataFrame({'Bias'           :  SimRes[:,i*7 + 1], 'StdErr'         :  SimRes[:,i*7 + 2],\\\n",
    "                         'Coverage (ind)' :  SimRes[:,i*7 + 3], 'Coverage (1st)' :  SimRes[:,i*7 + 4],\n",
    "                         'Coverage (FG)'  :  SimRes[:,i*7 + 5], 'Squared Error'  :  SimRes[:,i*7 + 6]})                      \n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Monte Carlo summary statistics for density estimation\")\n",
    "        print(\"\")\n",
    "        print(SR.describe())\n",
    "\n",
    "        print(\"\")\n",
    "        print('Root mean squared error (RMSE) of density estimate')\n",
    "        print((SR['Squared Error'].mean())**(1/2))\n",
    "        print(\"\")\n",
    "        \n",
    "        Q = SR[['Bias']].quantile(q=[0.05,0.95])\n",
    "        print(\"\")\n",
    "        print('Robust estimate of standard deviation of density estimate')\n",
    "        print((Q['Bias'][0.95]-Q['Bias'][0.05])/(2*1.645)) \n",
    "        print(\"\")\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "    design += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "html {\n",
       "  font-size: 62.5% !important; }\n",
       "body {\n",
       "  font-size: 1.5em !important; /* currently ems cause chrome bug misinterpreting rems on body element */\n",
       "  line-height: 1.6 !important;\n",
       "  font-weight: 400 !important;\n",
       "  font-family: \"Raleway\", \"HelveticaNeue\", \"Helvetica Neue\", Helvetica, Arial, sans-serif !important;\n",
       "  color: #222 !important; }\n",
       "\n",
       "div{ border-radius: 0px !important;  }\n",
       "div.CodeMirror-sizer{ background: rgb(244, 244, 248) !important; }\n",
       "div.input_area{ background: rgb(244, 244, 248) !important; }\n",
       "\n",
       "div.out_prompt_overlay:hover{ background: rgb(244, 244, 248) !important; }\n",
       "div.input_prompt:hover{ background: rgb(244, 244, 248) !important; }\n",
       "\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "  color: #333 !important;\n",
       "  margin-top: 0 !important;\n",
       "  margin-bottom: 2rem !important;\n",
       "  font-weight: 300 !important; }\n",
       "h1 { font-size: 4.0rem !important; line-height: 1.2 !important;  letter-spacing: -.1rem !important;}\n",
       "h2 { font-size: 3.6rem !important; line-height: 1.25 !important; letter-spacing: -.1rem !important; }\n",
       "h3 { font-size: 3.0rem !important; line-height: 1.3 !important;  letter-spacing: -.1rem !important; }\n",
       "h4 { font-size: 2.4rem !important; line-height: 1.35 !important; letter-spacing: -.08rem !important; }\n",
       "h5 { font-size: 1.8rem !important; line-height: 1.5 !important;  letter-spacing: -.05rem !important; }\n",
       "h6 { font-size: 1.5rem !important; line-height: 1.6 !important;  letter-spacing: 0 !important; }\n",
       "\n",
       "@media (min-width: 550px) {\n",
       "  h1 { font-size: 5.0rem !important; }\n",
       "  h2 { font-size: 4.2rem !important; }\n",
       "  h3 { font-size: 3.6rem !important; }\n",
       "  h4 { font-size: 3.0rem !important; }\n",
       "  h5 { font-size: 2.4rem !important; }\n",
       "  h6 { font-size: 1.5rem !important; }\n",
       "}\n",
       "\n",
       "p {\n",
       "  margin-top: 0 !important; }\n",
       "  \n",
       "a {\n",
       "  color: #1EAEDB !important; }\n",
       "a:hover {\n",
       "  color: #0FA0CE !important; }\n",
       "  \n",
       "code {\n",
       "  padding: .2rem .5rem !important;\n",
       "  margin: 0 .2rem !important;\n",
       "  font-size: 90% !important;\n",
       "  white-space: nowrap !important;\n",
       "  background: #F1F1F1 !important;\n",
       "  border: 1px solid #E1E1E1 !important;\n",
       "  border-radius: 4px !important; }\n",
       "pre > code {\n",
       "  display: block !important;\n",
       "  padding: 1rem 1.5rem !important;\n",
       "  white-space: pre !important; }\n",
       "  \n",
       "button{ border-radius: 0px !important; }\n",
       ".navbar-inner{ background-image: none !important;  }\n",
       "select, textarea{ border-radius: 0px !important; }\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This imports an attractive notebook style from Github\n",
    "from IPython.display import HTML\n",
    "from urllib.request import urlopen\n",
    "html = urlopen('http://bit.ly/1Bf5Hft')\n",
    "HTML(html.read().decode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
